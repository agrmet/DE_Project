{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687b8782-dd56-4ba1-a989-08e8ca996022",
   "metadata": {},
   "source": [
    "# Vulgarity Analysis in Reddit Posts by Authors\n",
    "\n",
    "**Author:** Agron Metaj\n",
    "\n",
    "**Date Created:** March 4, 2024  \n",
    "\n",
    "**Description:** This notebook analyzes the frequency and severity of vulgar language used by authors in a dataset of Reddit posts. Using a predefined list of profanities, it identifies top contributors to vulgar content across various subreddits. The analysis aims to highlight patterns in the use of vulgar language and may assist in content moderation efforts.\n",
    "\n",
    "**Output:**  \n",
    "- A list of authors ranked by the count of their vulgar posts and the average severity of vulgarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9565cfa-f106-4b88-9b48-d740907267f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 10:38:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, explode, split, col, lower, sum as spark_sum, first, broadcast, concat_ws, udf, avg\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "#Init Spark session\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.193:7077\") \\\n",
    "        .appName(\"test\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Get Spark context\n",
    "spark_context = spark_session.sparkContext\n",
    "#Set log level to error\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d1ebaf-e53e-48b7-8eae-25522d887991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import profanity list\n",
    "profanity_unfiltered = spark_session.read.csv(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/profanity_en.csv\", \n",
    "                                         header='true', inferSchema='true')\n",
    "\n",
    "#Import reddit post dataset\n",
    "posts_unfiltered = spark_session.read.json(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/corpus-webis-tldr-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2f6ed6-de58-4774-935c-0f5e8a9576cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to clean and tokenize post content\n",
    "def clean_and_tokenize(content):\n",
    "    # Simple tokenization and lowercasing, customize as needed\n",
    "    tokens = re.split(r'\\W+', content.lower())\n",
    "    return [token for token in tokens if token]\n",
    "\n",
    "# Register the UDF in Spark\n",
    "clean_and_tokenize_udf = udf(clean_and_tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Preprocess Reddit posts to tokenize the content\n",
    "posts = posts_unfiltered.withColumn(\"tokens\", clean_and_tokenize_udf(col(\"content\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b855ec18-6f81-4d7e-92b2-13eb16afc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode tokens for joining with the profanity list\n",
    "posts_exploded = posts.select(\"author\", \"id\", explode(col(\"tokens\")).alias(\"token\"))\n",
    "\n",
    "# Filter profanity list to relevant columns and broadcast for efficient joining\n",
    "profanity_filtered = broadcast(profanity_unfiltered.select(\"text\", \"severity_rating\"))\n",
    "\n",
    "# Join exploded posts with profanity list on matching tokens\n",
    "joined = posts_exploded.join(profanity_filtered, posts_exploded.token == lower(profanity_filtered.text), \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91958ad8-45a2-4e9b-9085-fdf4411bfecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+\n",
      "|           author|vulgar_posts_count|  average_severity|\n",
      "+-----------------+------------------+------------------+\n",
      "|        [deleted]|            122183|1.5462271483014198|\n",
      "|     iamtotalcrap|               882|1.5453619909502254|\n",
      "|          DejaBoo|               314|1.6144329896907217|\n",
      "|          codayus|               231|1.4870848708487086|\n",
      "|       Shaper_pmp|               216|1.4568527918781728|\n",
      "|     josiahpapaya|               171|1.6237659963436928|\n",
      "|       pixis-4950|               171|1.5519736842105263|\n",
      "|   dinosaur_train|               161| 1.522788203753351|\n",
      "|           mauxly|               160| 1.589734513274336|\n",
      "|      Death_Star_|               150| 1.262992125984252|\n",
      "|  RamsesThePigeon|               149| 1.554983922829582|\n",
      "|      herman_gill|               143|1.5890109890109894|\n",
      "|       FrankManic|               142| 1.598224852071006|\n",
      "|       Stingray88|               141|1.7966244725738392|\n",
      "|   iamadogforreal|               139|1.4148437500000004|\n",
      "|BluepillProfessor|               138|1.5350383631713556|\n",
      "|     InternetFree|               126|1.4772585669781932|\n",
      "|        redweasel|               126| 1.323921568627451|\n",
      "|  Nightmathzombie|               125|1.4825581395348841|\n",
      "|           dsprox|               120|1.7609756097560976|\n",
      "+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate vulgarity metrics by author\n",
    "vulgarity_by_author = joined.groupBy(\"author\")\\\n",
    "                            .agg(countDistinct(\"id\").alias(\"vulgar_posts_count\"), \n",
    "                                 avg(\"severity_rating\").alias(\"average_severity\"))\n",
    "\n",
    "# Order by vulgar post count and severity for top contributors\n",
    "top_vulgar_authors = vulgarity_by_author.orderBy(col(\"vulgar_posts_count\").desc(), col(\"average_severity\").desc())\n",
    "\n",
    "# Display top vulgar authors\n",
    "top_vulgar_authors.show()\n",
    "\n",
    "# Cleanup\n",
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
