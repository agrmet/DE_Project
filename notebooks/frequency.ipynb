{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51569238-713d-4e12-a45f-6b7271ed97f8",
   "metadata": {},
   "source": [
    "# Frequency Analysis of Subreddits\n",
    "\n",
    "**Author:** Noah Wassberg\n",
    "\n",
    "**Date Created:** March 7, 2024\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook analyzes the average amount of vulgarity per post in subreddits. This is done by using the reddit post dataset and the Surge AI profanity list dataset. The analysis aims to find which subreddits on average have the most/least explicit language in text content.\n",
    "\n",
    "## Output\n",
    "\n",
    "The output of this analysis is 2 lists of subreddits ranked by the average number of explicit language per post. Each subreddit is evaluated based on the number of explicit words divided by the number of posts from that subreddit, as determined by matching words and phrases against the Surge AI profanity list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a6dc4b-ae01-4f86-a5f0-7dbd023dc657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/07 09:00:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, explode, split, col, lower, sum as spark_sum, first, broadcast, concat_ws\n",
    "\n",
    "#Init Spark session\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.193:7077\") \\\n",
    "        .appName(\"test\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Get Spark context\n",
    "spark_context = spark_session.sparkContext\n",
    "#Set log level to error\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945c8bae-08a6-4627-a621-3e65256c5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import profanity list\n",
    "profanity_unfiltered = spark_session.read.csv(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/profanity_en.csv\", \n",
    "                                         header='true', inferSchema='true')\n",
    "\n",
    "#Import reddit post dataset\n",
    "posts_unfiltered = spark_session.read.json(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/corpus-webis-tldr-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f02de72-68d9-44f4-9f2b-5358385ee5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary fields from profanity list.\n",
    "profanity = profanity_unfiltered.drop(\"canonical_form_1\", \"canonical_form_2\", \"canonical_form_3\",\n",
    "                          \"category_1\",\"category_2\",\"category_3\",\"severity_description\", \"severity_rating\")\n",
    "\n",
    "#Remove unnecessary fields from reddit post dataset.\n",
    "posts_unfiltered = posts_unfiltered.drop(\"summary\",\"summary_len\",\"content_len\",\"author\", \"body\",\"normalizedBody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe4a832-0e8f-4f2c-ae42-697f7bdabf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the count of unique posts for each subreddit in the dataset\n",
    "#and remove subredddits with fewer than 200 posts to avoid outliers\n",
    "post_counts = posts_unfiltered.groupBy(\"subreddit_id\").agg(countDistinct(\"id\").alias(\"total_posts\")) \\\n",
    "    .filter(col(\"total_posts\") >= 200)\n",
    "\n",
    "#Remove all posts from subreddits with fewar than 200 posts                                 \n",
    "posts = posts_unfiltered.join(broadcast(post_counts), \"subreddit_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de64b009-dfad-458b-9a7d-53144993d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the content of each reddit post and the profanity list\n",
    "\n",
    "#Combine 'content' and 'title' into a single column for tokenization\n",
    "posts_combined = posts.withColumn(\"combined\", concat_ws(\" \", col(\"content\"), col(\"title\")))\n",
    "\n",
    "#Tokenize the combined content and title and convert to lowercase\n",
    "posts_tokenized = posts_combined.withColumn(\"word\", explode(split(lower(col(\"combined\")), \"\\\\s+\")))\n",
    "\n",
    "#Lowercase the 'text' in profanity DataFrame for case-insensitive matching\n",
    "profanity = profanity.withColumn(\"text\", lower(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f90039f-8ca0-478f-b6fe-963793e9612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the tokenized posts with the profanity DataFrame on the word matching 'text'\n",
    "#Using a broadcast join since profanity is much smaller than posts_tokenized\n",
    "joined_df = posts_tokenized.join(broadcast(profanity), posts_tokenized.word == profanity.text)\n",
    "\n",
    "\n",
    "\n",
    "#Group by subreddit, count explicit words, and rename the count column.\n",
    "explicit_count = joined_df.groupBy(\"subreddit_id\",\"subreddit\").count().withColumnRenamed(\"count\", \"explicit_word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26e23efb-4df3-481d-9d4c-9036882fafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join with post counts, calculate average explicit words per post, and select relevant columns.\n",
    "avg_explicit_count = post_counts.join(explicit_count, \"subreddit_id\") \\\n",
    "    .select(\"subreddit\", (col(\"explicit_word_count\") / col(\"total_posts\")).alias(\"average_explicit_words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b6a654-7833-4eeb-a6ec-87567195182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 least severe subreddits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|      subreddit|average_explicit_words|\n",
      "+---------------+----------------------+\n",
      "|   HomeworkHelp|  0.016826923076923076|\n",
      "|    learnpython|  0.017057569296375266|\n",
      "|    rocketbeans|   0.02240325865580448|\n",
      "|   TheSilphRoad|   0.02710843373493976|\n",
      "|        grammar|  0.029556650246305417|\n",
      "|latterdaysaints|  0.029595015576323987|\n",
      "|     dogemining|  0.030837004405286344|\n",
      "|       churning|   0.03355704697986577|\n",
      "|          italy|   0.03477218225419664|\n",
      "|       Toontown|  0.037815126050420166|\n",
      "|       chromeos|  0.040697674418604654|\n",
      "|        Unity3D|  0.044585987261146494|\n",
      "|           Hair|  0.044897959183673466|\n",
      "|         spacex|  0.045454545454545456|\n",
      "|      Wordpress|   0.04556962025316456|\n",
      "|     mtgfinance|  0.045871559633027525|\n",
      "|    freemasonry|   0.04669260700389105|\n",
      "|        pkmntcg|              0.046875|\n",
      "|       portugal|  0.048223350253807105|\n",
      "|   empirepowers|   0.04924242424242424|\n",
      "+---------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Top 20 most severe subreddits\n",
      "+-------------------+----------------------+\n",
      "|          subreddit|average_explicit_words|\n",
      "+-------------------+----------------------+\n",
      "|    gonewildstories|    11.215189873417721|\n",
      "|          newjersey|     5.888888888888889|\n",
      "|   neckbeardstories|     3.917562724014337|\n",
      "|               rant|    3.8365831012070566|\n",
      "|             incest|    2.7315436241610738|\n",
      "|     TrueOffMyChest|     2.654166666666667|\n",
      "|       badroommates|    2.6245847176079735|\n",
      "|         ProRevenge|    2.5784114052953155|\n",
      "|   fatpeoplestories|      2.56394640682095|\n",
      "|RandomActsOfBlowJob|    2.4430894308943087|\n",
      "|     marriedredpill|    2.3079019073569484|\n",
      "|cripplingalcoholism|     2.279532967032967|\n",
      "|        breakingmom|    2.2547169811320753|\n",
      "|       weeabootales|    2.2409638554216866|\n",
      "|  howtonotgiveafuck|                2.2025|\n",
      "|        Braveryjerk|    2.0849056603773586|\n",
      "|              MGTOW|    2.0591133004926108|\n",
      "|          JUSTNOMIL|    1.9738134206219313|\n",
      "|         TheRedPill|    1.9650110597225015|\n",
      "|         offmychest|     1.935604075691412|\n",
      "+-------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[subreddit: string, average_explicit_words: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cache the DataFrame to avoid recomputations\n",
    "avg_explicit_count.cache()\n",
    "\n",
    "#Show the 20 least explicit subreddits\n",
    "print(\"Top 20 least severe subreddits\")\n",
    "avg_explicit_count.orderBy(col(\"average_explicit_words\").asc()).show()\n",
    "#Show the 20 most explicit subreddits\n",
    "print(\"Top 20 most severe subreddits\")\n",
    "avg_explicit_count.orderBy(col(\"average_explicit_words\").desc()).show()\n",
    "\n",
    "#clear the cache\n",
    "avg_explicit_count.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cfd428-668b-49bb-b337-8eb3a9465a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
