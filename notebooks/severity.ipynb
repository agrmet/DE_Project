{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d61999b-7431-49b6-998e-e03bf87bc3e6",
   "metadata": {},
   "source": [
    "# Vulgarity Analysis of Subreddits\n",
    "\n",
    "**Author:** Noah Wassberg\n",
    "\n",
    "**Date Created:** March 4, 2024\n",
    "\n",
    "**Description:** This notebook analyzes the average severity of posts in subreddits. This is done by using the reddit post dataset and the surge AI profanity list dataset. The analysis aims to find which subreddits on average has the most vulgar text content.\n",
    "\n",
    "**Output:**\n",
    "- A list of subreddits ranked by the average severity in language of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69aa8e5-68ec-4132-aa6f-6cb0c56f58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 09:40:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, explode, split, col, lower, sum as spark_sum, first, broadcast, concat_ws\n",
    "\n",
    "#Init Spark session\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.193:7077\") \\\n",
    "        .appName(\"test\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Get Spark context\n",
    "spark_context = spark_session.sparkContext\n",
    "#Set log level to error\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ee6816-696e-448a-814d-c87e294c967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import profanity list\n",
    "profanity_unfiltered = spark_session.read.csv(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/profanity_en.csv\", \n",
    "                                         header='true', inferSchema='true')\n",
    "\n",
    "#Import reddit post dataset\n",
    "posts_unfiltered = spark_session.read.json(\"hdfs://192.168.2.193:9000/user/hadoop/input/input/corpus-webis-tldr-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272fd379-a739-4bf3-a094-ddd9e42240ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary fields from profanity list.\n",
    "profanity = profanity_unfiltered.drop(\"canonical_form_1\", \"canonical_form_2\", \"canonical_form_3\",\n",
    "                          \"category_1\",\"category_2\",\"category_3\",\"severity_description\")\n",
    "\n",
    "#Remove unnecessary fields from reddit post dataset.\n",
    "posts_unfiltered = posts_unfiltered.drop(\"summary\",\"summary_len\",\"content_len\",\"author\",\"body\",\"normalizedBody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf30eb7-5241-4a2e-9348-e767fc9a471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the count of unique posts for each subreddit in the dataset\n",
    "#and remove subredddits with fewer than 200 posts to avoid outliers\n",
    "post_counts = posts_unfiltered.groupBy(\"subreddit_id\").agg(countDistinct(\"id\").alias(\"total_posts\")) \\\n",
    "    .filter(col(\"total_posts\") >= 200)\n",
    "\n",
    "#Remove all posts from subreddits with fewar than 200 posts                                 \n",
    "posts = posts_unfiltered.join(broadcast(post_counts), \"subreddit_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77f309f-4e72-4c6b-9633-b50f7fc040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the content of each reddit post and the profanity list\n",
    "\n",
    "#Combine 'content' and 'title' into a single column for tokenization\n",
    "posts_combined = posts.withColumn(\"combined\", concat_ws(\" \", col(\"content\"), col(\"title\")))\n",
    "\n",
    "#Tokenize the combined content and title and convert to lowercase\n",
    "posts_tokenized = posts_combined.withColumn(\"word\", explode(split(lower(col(\"combined\")), \"\\\\s+\")))\n",
    "\n",
    "#Lowercase the 'text' in profanity DataFrame for case-insensitive matching\n",
    "profanity = profanity.withColumn(\"text\", lower(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa113b58-72a7-4ff7-a1a3-6aa74877f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the tokenized posts with the profanity DataFrame on the word matching 'text'\n",
    "#Using a broadcast join since profanity is much smaller than posts_tokenized\n",
    "joined_df = posts_tokenized.join(broadcast(profanity), posts_tokenized.word == profanity.text)\n",
    "\n",
    "#Note: sparks sum function is imported as spark_sum to avoid ambiguity\n",
    "#Aggregate the severity ratings for each word within each subreddit\n",
    "severity_totals_per_subreddit = joined_df.groupBy(\"subreddit_id\") \\\n",
    "                                         .agg(first(\"subreddit\").alias(\"subreddit\"), \n",
    "                                              spark_sum(\"severity_rating\").alias(\"total_severity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11b31e8-bcb4-41ae-8607-802a0977b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join 'severity_totals_per_subreddit' with 'post_counts' on 'subreddit_id'\n",
    "#calculate average severity per post for each subreddit\n",
    "#and select relevant columns\n",
    "avg_severity_per_post = severity_totals_per_subreddit.join(post_counts, \"subreddit_id\") \\\n",
    "    .withColumn(\"avg_severity\", col(\"total_severity\") / col(\"total_posts\")) \\\n",
    "    .select(\"subreddit_id\", \"subreddit\", \"avg_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ead0eb-b745-4dbd-8dc5-908cfb089fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 least severe subreddits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------------------+\n",
      "|subreddit_id|      subreddit|        avg_severity|\n",
      "+------------+---------------+--------------------+\n",
      "|    t5_2r6f3|   HomeworkHelp|0.021634615384615384|\n",
      "|    t5_2r8ot|    learnpython|0.024733475479744135|\n",
      "|    t5_2uas2|latterdaysaints| 0.03271028037383177|\n",
      "|    t5_3c2d7|   TheSilphRoad| 0.03433734939759036|\n",
      "|    t5_2qhwy|        grammar|0.034482758620689655|\n",
      "|    t5_2zn9o|    rocketbeans| 0.03747454175152749|\n",
      "|    t5_2zgq3|     dogemining|0.039647577092511016|\n",
      "|    t5_2vrf0|       churning| 0.04832214765100671|\n",
      "|    t5_2qmsf|           Hair| 0.04897959183673469|\n",
      "|    t5_2r1wh|       chromeos|0.049999999999999996|\n",
      "|    t5_2qkhk|          italy|0.052278177458033585|\n",
      "|    t5_2srow|         spacex| 0.05649350649350649|\n",
      "|    t5_2qhjq|      Wordpress|0.056708860759493676|\n",
      "|    t5_2vv1m|     mtgfinance|0.057492354740061154|\n",
      "|    t5_2qwj8|        Unity3D| 0.05817409766454353|\n",
      "|    t5_2qm6c|        crochet| 0.06063348416289592|\n",
      "|    t5_2te6p|WaltDisneyWorld|  0.0611353711790393|\n",
      "|    t5_2sn6d|       stunfisk|0.061244019138755976|\n",
      "|    t5_2qn38|    freemasonry| 0.06147859922178989|\n",
      "|    t5_2qknj|        arduino| 0.06254681647940076|\n",
      "+------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Top 20 most severe subreddits\n",
      "+------------+-------------------+------------------+\n",
      "|subreddit_id|          subreddit|      avg_severity|\n",
      "+------------+-------------------+------------------+\n",
      "|    t5_2rvlj|    gonewildstories|16.365822784810128|\n",
      "|    t5_2qhsu|          newjersey|11.606111111111106|\n",
      "|    t5_2qn2b|               rant| 6.891086350974926|\n",
      "|    t5_2waxs|   neckbeardstories| 6.291756272401434|\n",
      "|    t5_2yuqy|     TrueOffMyChest| 4.610833333333334|\n",
      "|    t5_2w1n0|       badroommates| 4.162126245847177|\n",
      "|    t5_2vzax|   fatpeoplestories| 4.127080795777504|\n",
      "|    t5_2qx19|             incest| 4.093959731543626|\n",
      "|    t5_2s68b|cripplingalcoholism|3.9622252747252764|\n",
      "|    t5_2vjvl|         ProRevenge|3.9556008146639523|\n",
      "|    t5_2tna8|  howtonotgiveafuck| 3.940249999999998|\n",
      "|    t5_2y6ap|        breakingmom|3.8782161234991417|\n",
      "|    t5_2wnp3|       weeabootales|3.5839357429718883|\n",
      "|    t5_2tpfa|RandomActsOfBlowJob|3.5211382113821137|\n",
      "|    t5_32dvh|     marriedredpill|3.4779291553133516|\n",
      "|    t5_2sjgc|              MGTOW|  3.36847290640394|\n",
      "|    t5_2ranw|         offmychest|3.2983988355167466|\n",
      "|    t5_2tl1h|        Braveryjerk| 3.267924528301886|\n",
      "|    t5_377ps|          JUSTNOMIL|3.1983633387888704|\n",
      "|    t5_2ve1u|         TheRedPill|3.1965815403177142|\n",
      "+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[subreddit_id: string, subreddit: string, avg_severity: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cache the DataFrame to avoid recomputations\n",
    "avg_severity_per_post.cache()\n",
    "\n",
    "#Show the 20 least severe subreddits\n",
    "print(\"Top 20 least severe subreddits\")\n",
    "avg_severity_per_post.orderBy(col(\"avg_severity\").asc()).show()\n",
    "#Show the 20 most severe subreddits\n",
    "print(\"Top 20 most severe subreddits\")\n",
    "avg_severity_per_post.orderBy(col(\"avg_severity\").desc()).show()\n",
    "\n",
    "#clear the cache\n",
    "avg_severity_per_post.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63f401-fca5-47f9-b266-0db9a181026a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
